{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Score a PySpark model with IBM Streams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "This notebook uses Streams to deploy a [LinearRegression model](https://spark.apache.org/docs/2.2.0/api/python/pyspark.ml.html#pyspark.ml.regression.LinearRegression) built using the PySpark ML library. \n",
    "The model was created [in this notebook](https://github.com/IBM/db2-event-store-iot-analytics/blob/master/notebooks/Event_Store_ML_Model_Deployment.ipynb).\n",
    "\n",
    "\n",
    "The following code snippet was added to the aforementioned notebook to save the model:\n",
    "``` python\n",
    "saved_model_dir = \"path/to/model\"\n",
    "model.save(saved_model_dir)\n",
    "```\n",
    "You can view the original notebook to learn about the model and/or customize it further. \n",
    "Tested using pyspark 2.4.4 and Spark 2.4.4. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About the application \n",
    "The goal of this notebook is to load and score a Spark model on a stream of data. \n",
    "The scenario is that we have a stream of data of temperature readings from sensors. Each tuple or data item on the stream has this format:\n",
    "\n",
    "``` python\n",
    "{'device': 3,   'sensor': 18,   'ts': 1541019342143,   'ambient_temp': 26.30057257150144,\n",
    "  'power': 11.516388340597683,\n",
    "  'temperature': 44.02880734420622,\n",
    "  'prediction': 44.940489852384474}\n",
    "\n",
    "```    \n",
    "We want to predict the next temperature that the sensor will report using the model previously created.\n",
    "\n",
    "The result will be a new stream that includes all the data in the input and  a predicted temperature as an additional attribute:\n",
    "``` python\n",
    "{'device': 3,   'sensor': 18,   'ts': 1541019342143,   'ambient_temp': 26.30057257150144,\n",
    "  'power': 11.516388340597683,\n",
    "  'temperature': 44.02880734420622,\n",
    "  'prediction': 44.940489852384474}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the model\n",
    "\n",
    "As mentioned before, the model was saved using `model.save`. It has been uploaded to GitHub so download it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-11-26 20:01:35--  https://github.com/natashadsilva/scratch/blob/master/LinearModel.zip?raw=true\n",
      "Resolving github.com (github.com)... 192.30.253.113\n",
      "Connecting to github.com (github.com)|192.30.253.113|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://github.com/natashadsilva/scratch/raw/master/LinearModel.zip [following]\n",
      "--2019-11-26 20:01:35--  https://github.com/natashadsilva/scratch/raw/master/LinearModel.zip\n",
      "Reusing existing connection to github.com:443.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/natashadsilva/scratch/master/LinearModel.zip [following]\n",
      "--2019-11-26 20:01:36--  https://raw.githubusercontent.com/natashadsilva/scratch/master/LinearModel.zip\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 199.232.8.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|199.232.8.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 8630 (8.4K) [application/zip]\n",
      "Saving to: ‘lmodel.zip’\n",
      "\n",
      "100%[======================================>] 8,630       --.-K/s   in 0s      \n",
      "\n",
      "2019-11-26 20:01:36 (46.1 MB/s) - ‘lmodel.zip’ saved [8630/8630]\n",
      "\n",
      "Archive:  lmodel.zip\n",
      "   creating: model_dir/stages/\n",
      "   creating: model_dir/stages/2_LinearRegression_805e3b7fa929/\n",
      "   creating: model_dir/stages/2_LinearRegression_805e3b7fa929/data/\n",
      " extracting: model_dir/stages/2_LinearRegression_805e3b7fa929/data/_SUCCESS  \n",
      "  inflating: model_dir/stages/2_LinearRegression_805e3b7fa929/data/part-00000-1e853830-28b3-48c7-844f-8016a8ab8bba-c000.snappy.parquet  \n",
      "  inflating: model_dir/stages/2_LinearRegression_805e3b7fa929/data/.part-00000-1e853830-28b3-48c7-844f-8016a8ab8bba-c000.snappy.parquet.crc  \n",
      "   creating: model_dir/stages/2_LinearRegression_805e3b7fa929/metadata/\n",
      "  inflating: model_dir/stages/2_LinearRegression_805e3b7fa929/metadata/.part-00000.crc  \n",
      " extracting: model_dir/stages/2_LinearRegression_805e3b7fa929/metadata/_SUCCESS  \n",
      "  inflating: model_dir/stages/2_LinearRegression_805e3b7fa929/metadata/part-00000  \n",
      "   creating: model_dir/stages/0_VectorAssembler_2856bc39a790/\n",
      "   creating: model_dir/stages/0_VectorAssembler_2856bc39a790/metadata/\n",
      "  inflating: model_dir/stages/0_VectorAssembler_2856bc39a790/metadata/.part-00000.crc  \n",
      " extracting: model_dir/stages/0_VectorAssembler_2856bc39a790/metadata/_SUCCESS  \n",
      "  inflating: model_dir/stages/0_VectorAssembler_2856bc39a790/metadata/part-00000  \n",
      "   creating: model_dir/stages/1_StandardScaler_adc0dbcf59b4/\n",
      "   creating: model_dir/stages/1_StandardScaler_adc0dbcf59b4/data/\n",
      "  inflating: model_dir/stages/1_StandardScaler_adc0dbcf59b4/data/part-00000-e60c7bf7-a40e-4c90-a184-eb2d5f3f40d9-c000.snappy.parquet  \n",
      "  inflating: model_dir/stages/1_StandardScaler_adc0dbcf59b4/data/.part-00000-e60c7bf7-a40e-4c90-a184-eb2d5f3f40d9-c000.snappy.parquet.crc  \n",
      " extracting: model_dir/stages/1_StandardScaler_adc0dbcf59b4/data/_SUCCESS  \n",
      "   creating: model_dir/stages/1_StandardScaler_adc0dbcf59b4/metadata/\n",
      "  inflating: model_dir/stages/1_StandardScaler_adc0dbcf59b4/metadata/.part-00000.crc  \n",
      " extracting: model_dir/stages/1_StandardScaler_adc0dbcf59b4/metadata/_SUCCESS  \n",
      "  inflating: model_dir/stages/1_StandardScaler_adc0dbcf59b4/metadata/part-00000  \n",
      "   creating: model_dir/metadata/\n",
      "  inflating: model_dir/metadata/.part-00000.crc  \n",
      " extracting: model_dir/metadata/_SUCCESS  \n",
      "  inflating: model_dir/metadata/part-00000  \n"
     ]
    }
   ],
   "source": [
    "!mkdir -p model_dir\n",
    "!wget -O lmodel.zip https://github.com/natashadsilva/scratch/blob/master/LinearModel.zip?raw=true\n",
    "!unzip -o -d model_dir  lmodel.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and install PySpark\n",
    "\n",
    "Install PySpark from `pip`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/87/21/f05c186f4ddb01d15d0ddc36ef4b7e3cedbeb6412274a41f26b55a650ee5/pyspark-2.4.4.tar.gz (215.7MB)\n",
      "\u001b[K     |████████████████████████████████| 215.7MB 154kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting py4j==0.10.7 (from pyspark)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/53/c737818eb9a7dc32a7cd4f1396e787bd94200c3997c72c1dbe028587bd76/py4j-0.10.7-py2.py3-none-any.whl (197kB)\n",
      "\u001b[K     |████████████████████████████████| 204kB 46.5MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/dsxuser/.cache/pip/wheels/ab/09/4d/0d184230058e654eb1b04467dbc1292f00eaa186544604b471\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.7 pyspark-2.4.4\n"
     ]
    }
   ],
   "source": [
    "!pip install --user pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make sure Streams Python API is installed\n",
    "Make sure streamsx package is installed and at least version 1.13.14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: streamsx\r\n",
      "Version: 1.13.14\r\n",
      "Summary: IBM Streams Python Support\r\n",
      "Home-page: https://github.com/IBMStreams/pypi.streamsx\r\n",
      "Author: IBM Streams @ github.com\r\n",
      "Author-email: debrunne@us.ibm.com\r\n",
      "License: Apache License - Version 2.0\r\n",
      "Location: /opt/conda/envs/Python36/lib/python3.6/site-packages\r\n",
      "Requires: requests, future, dill\r\n",
      "Required-by: \r\n"
     ]
    }
   ],
   "source": [
    "!pip show streamsx\n",
    "# upgrade if needed\n",
    "# !pip install --upgrade streamsx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an instance of the Streaming analytics service\n",
    "\n",
    "The Streams application does not run in this notebook, rather it runs on a Streams instance. If you do not already have one, follow the [steps here](http://ibmstreams.github.io/streamsx.documentation/docs/python/1.6/python-appapi-devguide-2/#streams) to set up the Streaming Analytics service in IBM Cloud.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up a connection to the service\n",
    "\n",
    "<h4>Submit to the Streaming Analytics service</h4>\n",
    "        To connect to the Streaming Analytics service in IBM cloud you need to get the service credentials from the Streaming Analytics service dashboard.\n",
    "        <p>\n",
    "        To copy your service credentials, open the Streaming Analytics service dashboard click <strong>Service Credentials</strong>, then <strong>View Credentials</strong>, and copy the contents of the cell. Click <strong>Add new credentials</strong> if there are no credentials listed.\n",
    "        </p>\n",
    "        <p>See the image below for an example. Click to enlarge.</p>\n",
    "        <a href=\"https://developer.ibm.com/streamsdev/wp-content/uploads/sites/15/2019/11/sa-credentials-only.png\">\n",
    "        <img width=\"600\" height=\"500\" src=\"https://developer.ibm.com/streamsdev/wp-content/uploads/sites/15/2019/11/sa-credentials-only.png\"></a>\n",
    " <br/>\n",
    " <h4>Run the cell below and enter the credentials when prompted.</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming Analytics credentials:········\n"
     ]
    }
   ],
   "source": [
    "from streamsx.topology.context import ConfigParams\n",
    "from streamsx.topology import context\n",
    "import json\n",
    "import getpass\n",
    "\n",
    "\n",
    "service_cfg  = {}\n",
    "\n",
    "SA_credentials=getpass.getpass('Streaming Analytics credentials:')\n",
    "service_cfg[ConfigParams.SERVICE_DEFINITION] = json.loads(SA_credentials)\n",
    "\n",
    "def submit_topology(topo):\n",
    "    global service_cfg\n",
    "    service_cfg[context.ConfigParams.SSL_VERIFY] = False\n",
    "\n",
    "    # This specifies how the application will be deployed\n",
    "\n",
    "    contextType = context.ContextTypes.STREAMING_ANALYTICS_SERVICE\n",
    "\n",
    "    return context.submit (contextType, topo, config = service_cfg) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the class used to score the model\n",
    "\n",
    "The `LoadSparkModel` class is a callable class that will be used to load the model, and then score it on each incoming data item, or tuple.\n",
    "\n",
    "When the `__call__` method receives a tuple, it uses `model.transform` to get a prediction for the tuple and adds the result as a `prediction` attribute. \n",
    "\n",
    "The data is not scored in batches but in real-time, as it arrives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamsx.ec\n",
    "import time\n",
    "import random\n",
    "\n",
    "import warnings\n",
    "import logging\n",
    "import csv \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "class LoadSparkModel:\n",
    "    def __init__(self, model_file):\n",
    "        # name of the model \n",
    "        self.model_path = model_file\n",
    "    def __exit__(self, exc_type, exc_value, traceback):\n",
    "        pass\n",
    "    def __enter__(self):\n",
    "        # This function is called when the  application is starting on the Streams runtime,\n",
    "\n",
    "        # Pyspark requires that PYSPARK_PYTHON` should be set to the Python executable at  `PYTHONHOME`.  \n",
    "        # Since this app is running on a Streams instance in the cloud there's no access  to environment variables,\n",
    "        # So I worked around it by setting the environment variable programmatically, before importing the pyspark modules.\n",
    "\n",
    "        os.environ[\"PYSPARK_PYTHON\"]=os.environ[\"PYTHONHOME\"] + \"/bin/python\"\n",
    "        # Application is starting on the Streams runtime,\n",
    "        from pyspark.ml import PipelineModel\n",
    "        from pyspark.sql import SparkSession\n",
    "        \n",
    "\n",
    "        logging.getLogger(\"SparkModel\").info(\"INFO: Loading Spark Model\")\n",
    "        # load json and create model\n",
    "        try :\n",
    "            self.sparkSession = SparkSession.builder.appName(\"Score Spark Model with Streams\").getOrCreate()\n",
    "            #get path to model at runtime\n",
    "            path_to_model = streamsx.ec.get_application_directory() + \"/etc/\" +  self.model_path\n",
    "            self.sparkModel = PipelineModel.load(path_to_model)\n",
    "            logging.getLogger(\"SparkModel\").info(\"INFO: Successfully loaded ,odel\")\n",
    "        except Exception as e:\n",
    "            logging.getLogger(\"SparkModel\").error(\"ERROR loading file \" +  str(e))\n",
    "        \n",
    "    def __call__(self, tpl):\n",
    "        logging.getLogger(\"SparkModel\").info(\"INFO: Going to run model on tuple\")\n",
    "        # wrap tuple in a Data Frame \n",
    "        tpl_as_DF = self.sparkSession.createDataFrame([tpl])\n",
    "        # score model\n",
    "        result = self.sparkModel.transform(tpl_as_DF).collect()[0]\n",
    "        \n",
    "        logging.getLogger(\"SparkModel\").debug(\"INFO: Ran model on tuple\")\n",
    "        # add prediction to input tuple\n",
    "        \n",
    "        tpl[\"prediction\"] = result.prediction\n",
    "        return tpl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define data generation function\n",
    "\n",
    "For simplicity we'll use the `readings` function below to simulate a stream of data. This function generates a new tuple every 0.1 seconds.\n",
    "See the documentation  to [connect to other data sources](http://ibmstreams.github.io/streamsx.documentation/docs/python/1.6/python-appapi-devguide-4/#adapters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, time\n",
    "\n",
    "def readings():\n",
    "    while True:\n",
    "        time.sleep(0.1)\n",
    "        time_now = 1541019341*1000\n",
    "        record_processed = 0\n",
    "        deviceID = random.randint(1, 3)\n",
    "        sensorID = random.randint(1, 50)\n",
    "        ambient_temp = random.gauss(24.5, 2)  # ambient temp \n",
    "        time_now += random.randint(10,1500)\n",
    "        power = random.gauss(10, 3) # power consumption\n",
    "        noise = random.gauss(0,1.5)\n",
    "        temp = 1.3 * ambient_temp + 0.5 * power + 5 + noise\n",
    "\n",
    "        yield dict(device=int(deviceID), \n",
    "                   sensor=int(sensorID), ts=time_now, \n",
    "                   ambient_temp=float(ambient_temp),\n",
    "                   power=float(power), \n",
    "                   temperature=float(temp))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the Streams application\n",
    "\n",
    "A Streams application is called directed graph called a `Topology`.\n",
    "\n",
    "## 1. Create the Topology object \n",
    "\n",
    "First, create the Topology and set up the prerequisites:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-26 20:04:07,836 - __PROJECT_LIB__ - ERROR - ProjectHandle: Project ID neither provided nor found in the environment.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from streamsx.topology.topology import Topology\n",
    "import streamsx.topology.context\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "model_path = \"model_dir\" # directory we downloaded the model into\n",
    "\n",
    "topo = Topology(name=\"SparkScoring\")\n",
    "\n",
    "# This makes sure the model files are available to the application at runtime\n",
    "\n",
    "topo.add_file_dependency(model_path + \"/\", \"etc\") \n",
    "\n",
    "\n",
    "# Pyspark requires the same version of pyspark as used in the notebook on the Streams host. \n",
    "# For Streams running in the cloud, work around this by including  pyspark in the compiled application using `topo.add_pip_package`.\n",
    "\n",
    "pyspark_version = !pip show pyspark\n",
    "pyspark_version = pyspark_version[1].split(':')[1].strip()\n",
    "topo.add_pip_package(\"pyspark==\" + pyspark_version)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create the source stream\n",
    "\n",
    "Use the `readings` function to create our input `Stream`. The Stream class represents a potentially infinite sequence of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The source_stream will contain the tuples generated by the readings function\n",
    "\n",
    "source_stream = topo.source(readings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Use the `LoadSparkModel` class to score the data from the source stream\n",
    "\n",
    "This is the core of the application. \n",
    "\n",
    "- We want to convert every tuple on the source Stream to a new Stream that contains the predictions.\n",
    "- We already have the `LoadSparkModel` class that will return a prediction, given a tuple.\n",
    "\n",
    "\n",
    "Use the `map` transform to put it all together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_scorer = LoadSparkModel(model_path)\n",
    "predictions_stream = source_stream.map(model_scorer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create a view to see output from the notebook\n",
    "A `View` is a connection to the running application that allows you to see the data on a particular Stream.\n",
    "\n",
    "\n",
    "Create a view on the `predictions` stream to see the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "predictions_stream.print()\n",
    "\n",
    "# create a view to watch the stream data while running\n",
    "\n",
    "\n",
    "results_view = predictions_stream.view(name=\"Predictions\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit topology\n",
    "\n",
    "As mentioned, the Streams application does not run in this notebook, rather it runs on the Streams instance. So this cell submits the application to the instance for execution, using the `submit_topology` function defined earlier.\n",
    "\n",
    "\n",
    "**Note:** If you see messages like this: `_IntProgress(value=0, bar_style='info', description='Initializing', max=10, style=ProgressStyle(description_wid…_`\n",
    "This is because ipywidgets are not enabled in your kernel. They're currently not supported in Watson Studio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitting Topology to Streams for execution..\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b2e2502f241443996f41f2f610f95ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, bar_style='info', description='Initializing', max=10, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JobId:  0 \n",
      "Job name:  notebook::SparkScoring_0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# The submission_result object contains information about the running application, or job\n",
    "print(\"Submitting Topology to Streams for execution..\")\n",
    "submission_result = submit_topology(topo)\n",
    "\n",
    "if submission_result.job:\n",
    "    streams_job = submission_result.job\n",
    "    print (\"JobId: \", streams_job.id , \"\\nJob name: \", streams_job.name)\n",
    "else:\n",
    "    print(\"Submission failed: \"   + str(submssion_result))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to the view to see the data\n",
    "\n",
    "Once the application is running, run this cell to see some output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching data from the view\n",
      "Fetched 5 result tuples\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ambient_temp</th>\n",
       "      <th>device</th>\n",
       "      <th>power</th>\n",
       "      <th>prediction</th>\n",
       "      <th>sensor</th>\n",
       "      <th>temperature</th>\n",
       "      <th>ts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>21.599243</td>\n",
       "      <td>1</td>\n",
       "      <td>6.839050</td>\n",
       "      <td>36.513092</td>\n",
       "      <td>22</td>\n",
       "      <td>37.315819</td>\n",
       "      <td>1541019342371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22.755869</td>\n",
       "      <td>2</td>\n",
       "      <td>19.986368</td>\n",
       "      <td>44.562062</td>\n",
       "      <td>10</td>\n",
       "      <td>43.803365</td>\n",
       "      <td>1541019341967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25.470143</td>\n",
       "      <td>2</td>\n",
       "      <td>14.585983</td>\n",
       "      <td>45.392438</td>\n",
       "      <td>40</td>\n",
       "      <td>49.973616</td>\n",
       "      <td>1541019342335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>22.982050</td>\n",
       "      <td>3</td>\n",
       "      <td>12.470996</td>\n",
       "      <td>41.111915</td>\n",
       "      <td>27</td>\n",
       "      <td>41.968606</td>\n",
       "      <td>1541019341880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>29.672598</td>\n",
       "      <td>1</td>\n",
       "      <td>13.865026</td>\n",
       "      <td>50.483845</td>\n",
       "      <td>50</td>\n",
       "      <td>49.462162</td>\n",
       "      <td>1541019342027</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ambient_temp  device      power  prediction  sensor  temperature  \\\n",
       "0     21.599243       1   6.839050   36.513092      22    37.315819   \n",
       "1     22.755869       2  19.986368   44.562062      10    43.803365   \n",
       "2     25.470143       2  14.585983   45.392438      40    49.973616   \n",
       "3     22.982050       3  12.470996   41.111915      27    41.968606   \n",
       "4     29.672598       1  13.865026   50.483845      50    49.462162   \n",
       "\n",
       "              ts  \n",
       "0  1541019342371  \n",
       "1  1541019341967  \n",
       "2  1541019342335  \n",
       "3  1541019341880  \n",
       "4  1541019342027  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    print(\"Fetching data from the view\")\n",
    "    \n",
    "    data_queue = results_view.start_data_fetch()\n",
    "    tpls = []\n",
    "    num_to_fetch = 5\n",
    "    for i in range(num_to_fetch):\n",
    "        tpls.append(data_queue.get())\n",
    "        \n",
    "    print(\"Fetched \" + str(len(tpls)) + \" result tuples\")\n",
    "finally:\n",
    "    results_view.stop_data_fetch()\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(tpls)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## See the job in the Streams Console\n",
    "See the running job's logs and other metrics in the Streams Console.\n",
    "The Streams Python development guide has [instructions to access the Streams console](http://ibmstreams.github.io/streamsx.documentation/docs/python/1.6/python-appapi-devguide-3/#42-see-job-status)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cancel the job\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_result.job.cancel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- [Map transform](http://ibmstreams.github.io/streamsx.documentation/docs/python/1.6/python-appapi-devguide-4/#map)\n",
    "- [Streams Python API Doc](https://streamsxtopology.readthedocs.io/en/stable/streamsx.topology.topology.html#stream-processing)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
