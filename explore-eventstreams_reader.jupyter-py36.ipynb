{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore Event Streams Reader\n",
    "\n",
    "This notebook shows how to read the Kafka topic published by the Streams job in a notebook, as an illustrative example.\n",
    "\n",
    "**This project contains Sample Materials, provided under license.  \n",
    "Licensed Materials - Property of IBM.  \n",
    "Â© Copyright IBM Corp. 2019. All Rights Reserved.  \n",
    "US Government Users Restricted Rights - Use, duplication or disclosure restricted by GSA ADP Schedule Contract with IBM Corp.**\n",
    "\n",
    "\n",
    "## Setup\n",
    "\n",
    "### Ensure appropriate python packages are available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install kafka-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create EventStreams Credentials File and Topic\n",
    "Place your Event Streams credentials file (JSON copied from the Event Streams service on the cloud; see the README notebook) in the `datasources/credentials/` directory under the project.  Update the EVENTSTREAMS_CREDENTIALS variable with its name.  For simplicity and consistency with other DataSource credentials files, naming it `eventstreams_USERID.json` is a good pattern.\n",
    "\n",
    "If you haven't already, create the Event Streams topic you'd like to use from the cloud Event Streams service interface, and set SIGNIFICANT_EVENTS_EVENTSTREAMS_TOPIC, below to match.\n",
    "This topic name must also match the topic name set in the `streaming_analytics_lfe_pipeline` notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "userName=getpass.getuser()\n",
    "\n",
    "EVENTSTREAMS_CREDENTIALS = os.environ['DSX_PROJECT_DIR']+'/datasources/credentials/eventstreams_'+userName+'.json' \n",
    "SIGNIFICANT_EVENTS_EVENTSTREAMS_TOPIC = \"SIGNIFICANT_LFE_SCORES\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to EventStreams with a new Consumer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kafka\n",
    "import json\n",
    "import ssl\n",
    "import time\n",
    "\n",
    "# Load credentials for the EventStreams\n",
    "with open(EVENTSTREAMS_CREDENTIALS) as f:\n",
    "    creds = json.load(f)\n",
    "\n",
    "# Connect to EventStreams, with our loaded credentials and attach to the requested Topic.\n",
    "cons = None\n",
    "while cons is None:\n",
    "    try:\n",
    "        cons = kafka.KafkaConsumer(SIGNIFICANT_EVENTS_EVENTSTREAMS_TOPIC, \\\n",
    "                                   bootstrap_servers=creds[\"kafka_brokers_sasl\"], \\\n",
    "                                   security_protocol=\"SASL_SSL\", \\\n",
    "                                   sasl_mechanism=\"PLAIN\", \\\n",
    "                                   sasl_plain_username=creds[\"user\"], \\\n",
    "                                   sasl_plain_password=creds[\"api_key\"], \\\n",
    "                                   ssl_cafile=ssl.get_default_verify_paths().cafile, \\\n",
    "                                   auto_offset_reset='latest')\n",
    "\n",
    "    except kafka.errors.NoBrokersAvailable:\n",
    "        print(\"No Brokers Available. Retrying ...\")\n",
    "        time.sleep(1)\n",
    "        cons = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Code for Interactive and Dynamic Widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "import threading\n",
    "import pandas as pd\n",
    "import collections\n",
    "import math\n",
    "\n",
    "# Handle button widget clicks, to start and stop the thread\n",
    "def toggler(button, out, restart, event):\n",
    "    if restart:\n",
    "        # Re-start the thread.  This will also set up the button for stopping.\n",
    "        t = threading.Thread(target=threadfunc, args=(button, out, event), name=\"Kafka Poller\")\n",
    "        t.start()\n",
    "    else:\n",
    "        # Ask the thread to quit.  This will also set up the button for restarting.\n",
    "        button.disabled = True\n",
    "        button.description = \"Stopping ...\"\n",
    "        event.set()\n",
    "\n",
    "old_cb = None\n",
    "\n",
    "# Actual thread code, to refresh the significant events table data periodically\n",
    "def threadfunc(button, out, event):\n",
    "    # Clear the event so somebody else has to ask us to stop again\n",
    "    event.clear()\n",
    "    \n",
    "    # Set the button's onclick to cancel behavior\n",
    "    button.disabled = True\n",
    "    button.description = \"Stop Updating\"\n",
    "    global old_cb\n",
    "    if old_cb is not None:\n",
    "        button.on_click(old_cb, remove=True)\n",
    "    old_cb = lambda w: toggler(button, out, False, event)\n",
    "    button.on_click(old_cb)\n",
    "    button.disabled = False\n",
    "\n",
    "    current_lines = collections.deque(maxlen=10)\n",
    "    \n",
    "    # Loop until asked to quit, fetching events\n",
    "    while not event.is_set():\n",
    "        global cons\n",
    "        try:\n",
    "            parts = cons.poll(10000, max_records=2)\n",
    "            for tp in parts:\n",
    "                for item in parts[tp]:\n",
    "                    m = json.loads(item.value.decode('ascii'))\n",
    "                    try:\n",
    "                        lhp = m[\"event\"][\"lfe_home_purchase\"][\"probabilities\"][0][1]\n",
    "                    except:\n",
    "                        lhp = math.nan\n",
    "                    try:\n",
    "                        lr = m[\"event\"][\"lfe_relocation\"][\"probabilities\"][0][1]\n",
    "                    except:\n",
    "                        lr = math.nan\n",
    "                        \n",
    "                    try:\n",
    "                        mstr = \"%-20s   %6d  %-10s  %-20s %-18.9f %-18.9f %s\\n\" % \\\n",
    "                            (time.strftime(\"%FT%TZ\", time.gmtime(item.timestamp/1000)), \\\n",
    "                             m[\"cust_id\"], \\\n",
    "                             m[\"event\"][\"sc_end_date\"], \\\n",
    "                             m[\"event\"][\"event_type_id\"], \\\n",
    "                             lhp, \\\n",
    "                             lr, \\\n",
    "                             m[\"message\"])\n",
    "                    except:\n",
    "                        mstr = str(m[\"event\"]) + \"\\n\"\n",
    "                    current_lines.append(mstr)\n",
    "            # Re-render\n",
    "            out.append_stdout(\"%-20s   %6s  %-10s  %-20s %-18s %-18s %s\\n\" % \\\n",
    "                              (\"Event Time\", \\\n",
    "                               \"CustId\", \\\n",
    "                               \"Scored To\", \\\n",
    "                               \"Last Event\", \\\n",
    "                               \"Home Purchase Prob\", \\\n",
    "                               \"Relocation Prob\", \\\n",
    "                               \"Significance\"))\n",
    "            out.append_stdout((\"=\"*160)+\"\\n\")\n",
    "            for x in reversed(current_lines):\n",
    "                out.append_stdout(x)\n",
    "            out.clear_output(wait=True)\n",
    "            time.sleep(0.25)\n",
    "        except Exception as e:\n",
    "            out.append_stderr(\"Got exception:\\n\")\n",
    "            out.append_stderr(str(e))\n",
    "            out.append_stderr(\"\\n\\n\")\n",
    "            raise\n",
    "    \n",
    "    # Set up the button for restarting\n",
    "    button.disabled = True\n",
    "    button.on_click(old_cb, remove=True)\n",
    "    old_cb = lambda w: toggler(button, out, True, event)\n",
    "    button.on_click(old_cb)\n",
    "    button.description = \"Start Updating\"\n",
    "    button.disabled = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Widget Layout and Background Thread Startup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an output widget, with an HTML label up top to say what it is, and a button top stop fetching events\n",
    "label = widgets.HTML(value='<h3>Live Significant Events</h3>')\n",
    "o = widgets.Output(layout={'border': '1px solid black', 'height': '220px'})\n",
    "b = widgets.Button(description='', button_style='danger', layout={'width': '25%'}, disabled=True)\n",
    "vbox = widgets.VBox([label, o, b])\n",
    "\n",
    "# Call the toggler function to (re)start the thread for us, with the event handle to control stopping the thread later.\n",
    "halt_event = threading.Event()\n",
    "toggler(b, o, True, halt_event)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Live Significant Events Feed\n",
    "\n",
    "Here we can watch as new Significant Events are generated by the Streams jobs.  New alerts show up at the top and only the 10 most recent alerts are shown, including the most recent scores for that customer, the significance of the alert, and information about the event that caused the customer to be re-scored resulting in the significant event.\n",
    "\n",
    "Signficant events are generated for a customer whenever that customer's score changes from below 40% to above 50%, or vice versa.  That is, moving from 55% to 45% back to 55% would not be seen as 'significant', and neither would moving from 35% to 45% and back to 35%.  However, for this demo, the first time a customer's score moves above 50%, even if it wasn't previously below 40%, a significant event will be generated.\n",
    "\n",
    "The 'significance' of a new customer score is determined in the `streaming_analytics_lfe_pipeline` notebook, in the `SignificantEvents` class definition.  Changing that cell to use a different definition of 'significance', and re-submitting the `significant_event_generation` Streams job from within that Notebook will change when these alerts are generated.  This could be a change to the thresholds, or a completely different way of determining 'signficance'.\n",
    "\n",
    "Below the significant events feed view is a button that will stop (and/or re-start) the background thread updating the feed from Eventstreams.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actually display the widgets\n",
    "display(vbox)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
