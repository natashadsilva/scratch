{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Streaming Analytics for Life Event Prediction\n",
    "\n",
    "This notebook provides the main streaming analytics pipeline that operationalizes the Life Event Prediction model in a real time data flow.  As new life events occur, they are passed through the model to determine whether they are signifcant enough to warrant action. \n",
    "\n",
    "The README notebook (README.ipynb) in this project provides context and instructions on configuring the context and executing this notebook. It also includes references to the Life Event Prediction model which is used to score customer interaction events in this notebook.\n",
    "\n",
    "**This project contains Sample Materials, provided under license.  \n",
    "Licensed Materials - Property of IBM.  \n",
    "Â© Copyright IBM Corp. 2019. All Rights Reserved.  \n",
    "US Government Users Restricted Rights - Use, duplication or disclosure restricted by GSA ADP Schedule Contract with IBM Corp.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration Variables\n",
    "\n",
    "The following variables can be adjusted to match your environment, according to how the Streams Add-on, remote data set (database connection), and Eventstreams/Kafka were configured (see the README notebook for details).  In particular, STREAMS_INSTANCE_NAME, CUSTOMER_SCORE_EVENTS_DATASET, EVENTSTREAMS_CREDENTIALS, and SIGNIFICANT_EVENTS_EVENTSTREAMS_TOPIC may need to change according to your setup.  If using the described default setup in the README notebook, the values below should work.  For the other variables, they shouldn't need to change at all, unless you are using a different set of live or historical events, or a different replay speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "# The streams instance where the jobs will be submitted\n",
    "STREAMS_INSTANCE_NAME = \"streams\"\n",
    "# The local dataset filename/path to replay \"real-time\" customer events from, and how fast to replay them (in events per second)\n",
    "# Note that the replay speed can be set much higher than the model can generate new scores, which will limit the effective\n",
    "# throughput of the system unless the scores can be sped up or parallelized.\n",
    "REAL_TIME_EVENTS_DATASET = os.environ['DSX_PROJECT_DIR'] + '/datasets/live_event.csv'\n",
    "REAL_TIME_EVENT_GENERATION_RATE = 2\n",
    "\n",
    "# The local dataset filename/path to load historical customer events from\n",
    "HISTORICAL_EVENTS_DATASET = os.environ['DSX_PROJECT_DIR'] + '/datasets/historical_event.csv'\n",
    "\n",
    "# The remote dataset to store new customer scores to.  Should be defined in the Project as a remote datasource and associated remote dataset.\n",
    "# If set to None, the job that saves the customer scores to the database will not be started.\n",
    "CUSTOMER_SCORE_EVENTS_DATASET = \"LFE_SCORES\"\n",
    "\n",
    "# The filename/path for the credentials to the remote EventStreams instance that significant events will be published to, and the topic to use.\n",
    "# If the credentials is set to None, or the topic is set to None, the job that publishes significant events to EventStreams will not be started.\n",
    "EVENTSTREAMS_CREDENTIALS = os.environ['DSX_PROJECT_DIR'] + '/datasources/credentials/eventstreams_' + getpass.getuser() + '.json'\n",
    "SIGNIFICANT_EVENTS_EVENTSTREAMS_TOPIC = \"SIGNIFICANT_LFE_SCORES\"\n",
    "\n",
    "# Streams Job Names for the various microservices\n",
    "REAL_TIME_EVENT_GENERATION_JOB_NAME = 'real_time_event_generation_' + getpass.getuser()\n",
    "SCORE_EVENTS_JOB_NAME = 'score_events_' + getpass.getuser()\n",
    "RECORD_SCORE_HISTORY_JOB_NAME = 'record_score_history_' + getpass.getuser()\n",
    "SIGNIFICANT_EVENT_GENERATION_JOB_NAME = 'significant_event_generation_' + getpass.getuser()\n",
    "PUBLISH_SIGNIFICANT_EVENTS_JOB_NAME = 'publish_significant_events_' + getpass.getuser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Module Installation\n",
    "\n",
    "As a convenience, if your ICP4D cluster can install Python modules using pip, the following cell can be executed to ensure the correct modules are installed and available.  Should only need to be done once per environment.  Alternatively, these modules can be installed into the ICP4D Python environment in some other way (see ICP4D documentation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell to potentially install required modules in the environment\n",
    "!pip install --upgrade --user streamsx --no-warn-script-location\n",
    "!pip install streamsx.database\n",
    "!pip install streamsx.eventstreams\n",
    "!pip install kafka-python\n",
    "\n",
    "!pip show streamsx\n",
    "\n",
    "!pip install --upgrade scikit-learn==0.21.2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Imports\n",
    "\n",
    "The following cell imports the needed Python modules for this notebook, including various general utilities, Streams support, and specific modules for executing the LFE models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell to prep the kernel, importing modules, doing basic housekeeping, setting helper variables from the environment, etc\n",
    "\n",
    "# General Imports\n",
    "import sys\n",
    "import os, requests, urllib3\n",
    "import getpass\n",
    "import io\n",
    "import csv\n",
    "import time\n",
    "import glob\n",
    "import shutil\n",
    "import json\n",
    "import collections\n",
    "import math\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Import Streams topology packages\n",
    "from streamsx.topology.topology import Topology\n",
    "from streamsx.topology import context\n",
    "from streamsx.topology.schema import StreamSchema\n",
    "\n",
    "# Import other Streams helper packages\n",
    "import streamsx.ec\n",
    "import streamsx.eventstreams as eventstreams\n",
    "import streamsx.database as db\n",
    "\n",
    "print(\"INFO: streamsx package version: \" + context.__version__)\n",
    "print(\"INFO: streamsx.database package version: \" + db.__version__)\n",
    "print(\"INFO: streamsx.eventstreams package version: \" + eventstreams.__version__)\n",
    "\n",
    "# Import scripts from this project\n",
    "if '../scripts' not in sys.path:\n",
    "    sys.path.insert(0, '../scripts')\n",
    "import StreamsScoringPipeline\n",
    "import memory\n",
    "import submit_support\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streams Instance Connection\n",
    "\n",
    "This cell connects to the Streams instance, and configures some basic parameters on that connection for later use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell to grab Streams instance config object and REST reference\n",
    "from icpd_core import icpd_util\n",
    "streams_cfg=icpd_util.get_service_instance_details(name=STREAMS_INSTANCE_NAME)\n",
    "\n",
    "import streamsx.rest as rest\n",
    "streams_cfg[context.ConfigParams.SSL_VERIFY] = False\n",
    "streams_instance = rest.Instance.of_service(streams_cfg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cancel Existing Streams Jobs\n",
    "\n",
    "The following cell can be executed to find all existing Streams jobs and cancel them, so you can re-execute the notebook from scratch and be sure the old jobs have been terminated before starting the new jobs or clearing the database tables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cancel existing jobs so that the there is a clean slate to start from\n",
    "for job in streams_instance.get_jobs():\n",
    "    if job.name.endswith(getpass.getuser()):\n",
    "        print(\"Cancelling existing job:\", job.name)\n",
    "        job.cancel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database Connection\n",
    "\n",
    "If using a remote dataset to store customer score results, the following cell uses the information in the remote dataset and datasource to configure the connection to the database for later use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dsx_core_utils\n",
    "\n",
    "if CUSTOMER_SCORE_EVENTS_DATASET is not None:\n",
    "    # Setup remote DB connectors and credentials\n",
    "    remote_dataSet = None\n",
    "    remote_dataSource = None\n",
    "    remote_db2credentials = None\n",
    "    remote_table_name = None\n",
    "    try:\n",
    "        remote_dataSet = dsx_core_utils.get_remote_data_set_info(CUSTOMER_SCORE_EVENTS_DATASET)\n",
    "        remote_dataSource = dsx_core_utils.get_data_source_info(remote_dataSet['datasource'])\n",
    "        remote_db2credentials = { 'username': remote_dataSource['user'], 'password': remote_dataSource['password'], 'jdbcurl': remote_dataSource['URL'] }\n",
    "        remote_table_name = remote_dataSet['table']\n",
    "    except:\n",
    "        print(\"Unable to retrieve dataset or datasource information for the dataset given by CUSTOMER_SCORE_EVENTS_DATASET.  It may not be defined.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Database Tables\n",
    "\n",
    "If using a remote dataset to store customer scores, the following cell deletes the database table and re-creates it, with the correct schema.  In a more realistic scenario, you wouldn't want to delete the old data, since you would just be adding new scores to and existing table, but in this accelerator, the same live scores are replayed, so the data in the database would be duplicated if the jobs were re-executed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import database utilities\n",
    "import jaydebeapi\n",
    "\n",
    "# Easy way to drop and recreate the DB table\n",
    "if CUSTOMER_SCORE_EVENTS_DATASET is not None and remote_dataSource is not None:\n",
    "    if (sys.version_info >= (3, 0)):\n",
    "        conn = jaydebeapi.connect(remote_dataSource['driver_class'], remote_dataSource['URL'], {'user': remote_dataSource['user'], 'password': remote_dataSource['password'], 'clientProgramName': \"pipeline-prep-\" + getpass.getuser()})\n",
    "    else:\n",
    "        conn = jaydebeapi.connect(remote_dataSource['driver_class'], [remote_dataSource['URL'], remote_dataSource['user'], remote_dataSource['password']])\n",
    "\n",
    "    curs = conn.cursor()\n",
    "    try:\n",
    "        curs.execute('DROP TABLE ' + remote_table_name)\n",
    "    except jaydebeapi.DatabaseError:\n",
    "        print(\"Error dropping table.  Probably doesn't exist yet.\")\n",
    "    else:\n",
    "        print(\"Table dropped.\")\n",
    "    try:\n",
    "        curs.execute('CREATE TABLE ' + remote_table_name + ' (CUSTOMER_ID INTEGER NOT NULL, ERROR_CODE INTEGER, EVENT_DATE DATE, EVENT_TYPE VARCHAR(255), HOME_PURCHASE_PROB REAL, RELOCATE_PROB REAL, INSERTION_TIME TIMESTAMP NOT NULL)')\n",
    "        curs.execute('CREATE INDEX ' + remote_table_name + '_INDEX ON ' + remote_table_name + ' (CUSTOMER_ID)')\n",
    "    except Exception as e:\n",
    "        print(\"Problem creating table: \" + str(e))\n",
    "    else:\n",
    "        print(\"Table created.\")\n",
    "    curs.close()\n",
    "    conn.close()\n",
    "else:\n",
    "    print(\"Skipping DB Table creation/clearing, since CUSTOMER_SCORE_EVENTS_DATASET was None, or the dataset didn't exist.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streams Submission Helper Function\n",
    "\n",
    "The `submitToStreams()` helper function here cancels a job, if it is already running, and then re-builds the topology into a Streams bundle and submits the job to the Streams instance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definitions of general helper functions to help submit jobs, query jobs, query job health, cancel jobs, etc\n",
    "def submitToStreams(topology):\n",
    "    # Create local copy of the streams config so this can be thread-safe\n",
    "    local_streams_cfg = dict(streams_cfg)\n",
    "\n",
    "    # Cancel the job from the instance if it is already running...\n",
    "    for job in streams_instance.get_jobs():\n",
    "        if job.name == topology.name:\n",
    "            print(\"Cancelling old job:\", job.name)\n",
    "            job.cancel()\n",
    "    \n",
    "    # Set the job config\n",
    "    job_config = context.JobConfig(job_name = topology.name, tracing = \"debug\")\n",
    "    job_config.add(local_streams_cfg)\n",
    "    \n",
    "    # Actually submit the job\n",
    "    print(\"Building and submitting new job:\", topology.name)\n",
    "    submission_result = context.submit('DISTRIBUTED', topology, local_streams_cfg)\n",
    "    return submission_result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streams Operator Classes\n",
    "\n",
    "The following class definitions define the behavior of some Streams operators specific to the accelerator's needs.\n",
    "\n",
    "### CSVFileReader\n",
    "\n",
    "This operator class acts as a Streams source operator, reading in a CSV file from the bundle and emitting each line as a tuple.  This is used to replay the live events file.  A helper function, `delay()` is also defined, which is a used in the Streams topology as a trivial operator to put some timing delay between the tuples emitted from this source.\n",
    "\n",
    "Streams source operators must execute _outside_ the notebook environment. Hence the CSVFileReader class is defined in a python script submit_support.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### StoreEvents\n",
    "\n",
    "This operator class loads all the historical events into the operator's memory at operator/job startup time, and then stores each new customer event into the operator's memory as it flows through, enabling the LFE scoring model to have access to a customer's full event history to compute the scores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StoreEvents(object):\n",
    "    \"\"\"store each cust_id's events in it's own list\n",
    "    Notes:\n",
    "        The historical data is read in on the notebook side and populates the\n",
    "        memory.events on the Streams side during the __enter__(). Memory.events\n",
    "        is colocated, refer to <topology>.colocate([]).\n",
    "    Args:\n",
    "        history_csv : file of historical events [cust_id, date, event]\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, history_csv=None):\n",
    "        self.dh = pd.read_csv(history_csv)\n",
    "\n",
    "    def __enter__(self):\n",
    "        for historical in self.dh.values:\n",
    "            lst = list(historical)\n",
    "            memory.events[lst[0]].append(lst)\n",
    "\n",
    "    def __exit__(self, exc_type, exc_value, traceback):\n",
    "        pass  # exit required\n",
    "\n",
    "    def __call__(self, tuple: list):\n",
    "        \"\"\"store an event memory shared by multiple operators.\n",
    "        Note:\n",
    "            memory is shared between operators\n",
    "        Args:\n",
    "            tuple :  tuple is the event : [cust_id, date, event]\n",
    "        Return:\n",
    "            dict events\n",
    "        \"\"\"\n",
    "        memory.events[tuple[0]].append(tuple)\n",
    "\n",
    "        return {\"cust_id\": tuple[0], \"sc_end_date\": tuple[1], 'event_type_id': tuple[2]}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SignificantEvents\n",
    "\n",
    "This operator class is used to determine if a new customer score is \"Significant\" or not, based on a set of thresholds with a deadband in between, and whether that that customer's prior score was in the same threshold region.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SignificantEvents:\n",
    "    def __init__(self, high_threshold=0.50, low_threshold=0.40):\n",
    "        self.high_threshold = high_threshold\n",
    "        self.low_threshold = low_threshold\n",
    "        self.prior_alerts = collections.defaultdict(object)\n",
    "        self.idx = 0\n",
    "\n",
    "    def check_update_alert(self, tpl, lfe_class):\n",
    "        cust_id = tpl[\"cust_id\"]\n",
    "        old_alert = self.prior_alerts[cust_id][lfe_class]\n",
    "        \n",
    "        if tpl[lfe_class]['error_code'] > 0:\n",
    "            try:\n",
    "                cur_prob = tpl[lfe_class]['probabilities'][0][1]\n",
    "            except:\n",
    "                cur_prob = math.nan\n",
    "                \n",
    "            if cur_prob > self.high_threshold and (old_alert is None or old_alert is \"Low\"):\n",
    "                self.prior_alerts[cust_id][lfe_class] = \"High\"\n",
    "                return \"High\"\n",
    "            elif cur_prob < self.low_threshold and (old_alert is not None and old_alert is \"High\"):\n",
    "                self.prior_alerts[cust_id][lfe_class] = \"Low\"\n",
    "                return \"Low\"\n",
    "            else:\n",
    "                return None         \n",
    "        else:\n",
    "            # Ignore failed score events\n",
    "            return None\n",
    "        \n",
    "        \n",
    "    def __call__(self, tpl):\n",
    "        cust_id = tpl['cust_id']\n",
    "        ntuple = {'cust_id': cust_id, 'idx' : self.idx}\n",
    "        self.idx += 1\n",
    "        ntuple['event'] = tpl\n",
    "        \n",
    "        # A customer with no prior scores will alert only if a probability is above the threshold\n",
    "        if cust_id not in self.prior_alerts:\n",
    "            # Set the prior alerts to None for this customer\n",
    "            self.prior_alerts[cust_id] = {'lfe_home_purchase': None, 'lfe_relocation': None}\n",
    "        \n",
    "        # Check to see if we need to send any alerts, and update the prior alerts for next time\n",
    "        home_purchase_alert = self.check_update_alert(tpl, 'lfe_home_purchase')\n",
    "        relocation_alert = self.check_update_alert(tpl, 'lfe_relocation')\n",
    "        \n",
    "        if home_purchase_alert is None and relocation_alert is None:\n",
    "            # No alerts to send\n",
    "            return None\n",
    "        else:\n",
    "            # Something significant happened.\n",
    "            ntuple['message'] = \"\"\n",
    "            if home_purchase_alert is not None:\n",
    "                ntuple['message'] += \"Change to a \" + home_purchase_alert + \" Home Purchase Probability; \"\n",
    "            if relocation_alert is not None:\n",
    "                ntuple['message'] += \"Change to a \" + relocation_alert + \" Relocation Probability\"\n",
    "            return {'new':ntuple, 'all':self.prior_alerts}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streams Topology Creation\n",
    "\n",
    "The following cells define functions to create the various Streams Topology graphs for each Streams job.\n",
    "\n",
    "### Real-Time Event Generation\n",
    "\n",
    "This job replays the live events, and it's topology is simply a `CSVFileReader` source operator, delayed to the desired event generation rate, and published to a Streams topic `real_time_events`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createRealTimeEventGenerationTopology():\n",
    "    topo = Topology(REAL_TIME_EVENT_GENERATION_JOB_NAME)\n",
    "\n",
    "    # add files to be contained in the archive which is deployed to the node running the application\n",
    "    # in this sample we need the `dataset` with sample data to be present at the worker node\n",
    "    filename_in_bundle = topo.add_file_dependency(REAL_TIME_EVENTS_DATASET, 'etc')\n",
    "\n",
    "    # let the csv file reader the source/edge node in our topology, producing the 'records' stream                    \n",
    "    real_time_events_input = topo.source(submit_support.CSVFileReader(filename_in_bundle))\n",
    "\n",
    "    # Insert a delay between record replay\n",
    "    delay =  1.0/REAL_TIME_EVENT_GENERATION_RATE   #variable for lambda, not expression\n",
    "    real_time_events = real_time_events_input.filter(lambda t: time.sleep(float(delay)) or True)\n",
    "\n",
    "    real_time_events.view(name=\"real_time_events\", description=\"Real Time Customer Events\")\n",
    "\n",
    "    # publish results\n",
    "    real_time_events.publish(topic=\"real_time_events\")\n",
    "    \n",
    "    return topo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score Events\n",
    "\n",
    "This job performs the actual scoring of customer event histories, as each new event comes in.  It subscribes to the Streams topic `real_time_events`, and uses the `StoreEvents` operator to store them in operator memory for future scoring use, with the pre-loaded historical customer events, and then feeds them into the actual LFE model scoring code (defined in the `StreamsScoringPipeline.py` script).  Finally, the resulting customer scores are published to the Streams topic `new_scores`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createScoreEventsTopology():\n",
    "    # Stage some files needed by the model scoring code into the expected subdirectory needed by the scoring code, but that can still be added to the Streams bundle.\n",
    "    stage_directory = os.path.join(os.environ.get(\"DSX_PROJECT_DIR\"), \"datasets\", \"datasets\")\n",
    "    os.makedirs(stage_directory, exist_ok=True)\n",
    "    for stage_file in glob.glob(os.path.join(os.environ.get(\"DSX_PROJECT_DIR\"), \"datasets\", \"*.json\")):\n",
    "        staged = shutil.copy(stage_file, stage_directory)\n",
    "        print(\"Stage file '{}' for Streams\".format(staged))\n",
    "\n",
    "    topo = Topology(SCORE_EVENTS_JOB_NAME)\n",
    "    \n",
    "    topo.exclude_packages.add('pandas')\n",
    "    topo.add_pip_package('sklearn')\n",
    "    topo.add_pip_package('scipy')\n",
    "    \n",
    "    topo.add_file_dependency('../datasets/datasets', 'etc')  # Specify staged files to move.\n",
    "    \n",
    "    #load the history + real flow into memory\n",
    "    real_time_events = topo.subscribe(topic=\"real_time_events\")\n",
    "    \n",
    "    store_events = real_time_events.map(StoreEvents(history_csv=HISTORICAL_EVENTS_DATASET), name=\"store_events\")\n",
    "    store_events.view(name=\"store_events\", description=\"store real time events\")\n",
    "    \n",
    "    # score & save\n",
    "    new_scores = store_events.map(StreamsScoringPipeline.score_life_events(project_path=os.environ.get(\"DSX_PROJECT_DIR\")), name=\"new_scores\")\n",
    "    new_scores.view(name=\"new_scores\", description=\"New Customer Scores\")\n",
    "    new_scores.publish(topic=\"new_scores\")\n",
    "\n",
    "    ## colocate shared memory \n",
    "    store_events.colocate([store_events, new_scores])\n",
    "\n",
    "    return topo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Record Score History\n",
    "\n",
    "This job saves the customer scores into the database for off-line analysis of customer scores.  It subscribes to the Streams topic `new_scores`, converts the tuple schema to match the database table schema, and inserts the customer score into the database table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createRecordScoreHistoryTopology():\n",
    "    topo = Topology(RECORD_SCORE_HISTORY_JOB_NAME)\n",
    "\n",
    "    # SQL statements\n",
    "    sql_insert = 'INSERT INTO ' + remote_table_name + ' (CUSTOMER_ID, ERROR_CODE, EVENT_DATE, EVENT_TYPE, HOME_PURCHASE_PROB, RELOCATE_PROB, INSERTION_TIME) VALUES (?, ?, ?, ?, ?, ?, CURRENT TIMESTAMP)'\n",
    "    sql_select = 'SELECT * FROM ' + remote_table_name\n",
    "\n",
    "    new_scores = topo.subscribe(topic=\"new_scores\")\n",
    "    \n",
    "    # convert it to SPL schema for the database operator run_statement\n",
    "    tuple_schema = StreamSchema(\"tuple<int64 cust_id, int64 error_code, rstring event_date, rstring event_type_id, float64 home_prob, float64 reloc_prob>\")\n",
    "\n",
    "    def convert(tpl):\n",
    "        try:\n",
    "            result = (tpl[\"cust_id\"], \\\n",
    "                      tpl[\"error_code\"], \\\n",
    "                      tpl[\"sc_end_date\"], \\\n",
    "                      tpl[\"event_type_id\"], \\\n",
    "                      tpl[\"lfe_home_purchase\"][\"probabilities\"][0][1] if tpl[\"lfe_home_purchase\"][\"error_code\"] == 1 else None, \\\n",
    "                      tpl[\"lfe_relocation\"][\"probabilities\"][0][1] if tpl[\"lfe_relocation\"][\"error_code\"] == 1 else None)\n",
    "        except Exception as e:\n",
    "            print(\"Got exception: \", e, flush=True)\n",
    "            print(\"Tuple was: \", tpl, flush=True)\n",
    "            result = (tpl[\"cust_id\"], tpl[\"error_code\"], tpl[\"sc_end_date\"], tpl[\"event_type_id\"], None, None)\n",
    "        return result\n",
    "    \n",
    "    db_events = new_scores.map(convert, name=\"db_events\", schema=tuple_schema)\n",
    "    db_events.view(name=\"db_events\", description=\"storing events to db\")\n",
    "\n",
    "    insert_results = db.run_statement(name=\"INSERT\", stream=db_events, sql=sql_insert, sql_params=\"cust_id, error_code, event_date, event_type_id, home_prob, reloc_prob\" ,credentials = remote_db2credentials)\n",
    "\n",
    "    return topo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Significant Event Generation\n",
    "\n",
    "This job looks at new customer scores and determines if a new score is 'significant'.  It subscribes to the Streams topic `new_scores` and uses the `SignificantEvents` operator (defined above), to determine the significance of this score.  If it is significant, it publishes it to the Streams topic `significant_events`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createSignificantEventGenerationTopology():\n",
    "    topo = Topology(SIGNIFICANT_EVENT_GENERATION_JOB_NAME)\n",
    "\n",
    "    new_scores = topo.subscribe(topic=\"new_scores\")\n",
    "\n",
    "    full_significant_events = new_scores.map(SignificantEvents(), name=\"full_significant_events\")\n",
    "    full_significant_events.view( name=\"full_significant_events\", description=\"significant event and history\")\n",
    "\n",
    "    significant_events = full_significant_events.map(lambda t: t[\"new\"], name=\"significant_events\")\n",
    "    significant_events.view(name=\"significant_events\", description=\"significant event only\")\n",
    "    significant_events.publish(topic=\"significant_events\")\n",
    "    \n",
    "    return topo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Publish Significant Events\n",
    "\n",
    "This job publishes the significant scoring events to an external Kafka/Eventstreams bus, for use by some other business logic to use or display.  The topology here subscribes to the Streams topic `significant_events` and re-publishes to the chosen external Kafka topic.\n",
    "\n",
    "A similar streams job could be added, subscribing to the same Streams topic, to do some other filtering or actions based on these significant events, such as automatically sending an email, or doing deeper customer analysis when significant events have occurred for that customer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createPublishSignificantEventsTopology():\n",
    "    topo = Topology(PUBLISH_SIGNIFICANT_EVENTS_JOB_NAME)\n",
    "\n",
    "    significant_events = topo.subscribe(topic=\"significant_events\")\n",
    "    significant_events_as_json = significant_events.as_json()\n",
    "\n",
    "    with open(EVENTSTREAMS_CREDENTIALS) as f:\n",
    "        eventstreams.publish(significant_events_as_json, topic=SIGNIFICANT_EVENTS_EVENTSTREAMS_TOPIC, credentials=json.load(f))\n",
    "   \n",
    "    return topo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Job Submission\n",
    "\n",
    "In the following cells, the various Streams topologies are built and jobs submitted to the Streams instance for continual execution.  To speed up the topology building process, many of the jobs are built and submitted at once, using Python threads.  However, building and submitting the final job, to actually replay the live events, is delayed until all the main Streams jobs have been submitted, so that they are fully up and running before any new live events are fed into the system.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "\n",
    "# Thread executor so we can build and submit streams jobs in parallel.\n",
    "executor = concurrent.futures.ThreadPoolExecutor()\n",
    "futureset = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if EVENTSTREAMS_CREDENTIALS is not None and SIGNIFICANT_EVENTS_EVENTSTREAMS_TOPIC is not None:\n",
    "    sr1 = executor.submit(submitToStreams,createPublishSignificantEventsTopology())\n",
    "    futureset.add(sr1)\n",
    "    time.sleep(0.5)  # Sleep for a bit to give the thread a chance to start and create the progress widget in this cell\n",
    "else:\n",
    "    print(\"Skipping \" + PUBLISH_SIGNIFICANT_EVENTS_JOB_NAME + \" job creation and submission because either EVENTSTREAMS_CREDENTIALS or SIGNIFICANT_EVENTS_EVENTSTREAMS_TOPIC was set to None.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sr2 = executor.submit(submitToStreams,createSignificantEventGenerationTopology())\n",
    "futureset.add(sr2)\n",
    "time.sleep(0.5)  # Sleep for a bit to give the thread a chance to start and create the progress widget in this cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CUSTOMER_SCORE_EVENTS_DATASET is not None and remote_db2credentials is not None:\n",
    "    sr3 = executor.submit(submitToStreams,createRecordScoreHistoryTopology())\n",
    "    futureset.add(sr3)\n",
    "    time.sleep(0.5)  # Sleep for a bit to give the thread a chance to start and create the progress widget in this cell\n",
    "else:\n",
    "    print(\"Skipping \" + RECORD_SCORE_HISTORY_JOB_NAME + \" job creation and submission because CUSTOMER_SCORE_EVENTS_DATASET was None, or the dataset didn't exist.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sr4 = executor.submit(submitToStreams,createScoreEventsTopology())\n",
    "futureset.add(sr4)\n",
    "time.sleep(0.5)  # Sleep for a bit to give the thread a chance to start and create the progress widget in this cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is where we wait for the Streams jobs to finish build/submission before starting on the Real time Event Generation job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Barrier to wait for all the currently running submission jobs before we move on to start the final real-time event generation job, below\n",
    "concurrent.futures.wait(futureset)\n",
    "futureset.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sr5 = submitToStreams(createRealTimeEventGenerationTopology())\n",
    "if(sr5.job):\n",
    "    print(\"JobId: \", sr5.job.id, \" Name: \", sr5.job.name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Job Listing, Status, and Control\n",
    "\n",
    "The following cell defines some notebook widgets to display the currently submitted Streams jobs, with their status, and allows you to cancel them all, if you want.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "import threading\n",
    "import pandas as pd\n",
    "import time\n",
    "import getpass\n",
    "\n",
    "# Check for the job lists and their current health, and update the output\n",
    "def updater(ow):\n",
    "    df = pd.DataFrame(index=['Name','Health'])\n",
    "    for job in streams_instance.get_jobs():\n",
    "        if job.name.endswith(getpass.getuser()):\n",
    "            df[job.id] = [job.name, job.health]\n",
    "    ow.append_display_data(df.transpose())\n",
    "    ow.clear_output(wait=True)\n",
    "\n",
    "# The basic thread loop\n",
    "def threadfunc(ow):\n",
    "    while(True):\n",
    "        updater(ow)\n",
    "        time.sleep(5)\n",
    "\n",
    "# Respond when they ask to cancel the jobs\n",
    "def canceller(button, ow):\n",
    "    button.disabled = True\n",
    "    button.description = 'Cancellation in Progress ...'\n",
    "    for job in streams_instance.get_jobs():\n",
    "        if job.name.endswith(getpass.getuser()):\n",
    "            try:\n",
    "                job.cancel()\n",
    "            except:\n",
    "                pass\n",
    "    updater(ow)\n",
    "    button.description = 'Cancel All Jobs'\n",
    "    button.disabled = False\n",
    "    \n",
    "# Layout the widgets\n",
    "label = widgets.HTML(value='<h3>Current State of Running Streams Jobs</h3>')\n",
    "o = widgets.Output(layout={'border': '1px solid black'})\n",
    "b = widgets.Button(description='Cancel All Jobs', button_style='danger', layout={'width': '25%'})\n",
    "b.on_click(lambda w: canceller(w, o))\n",
    "vbox = widgets.VBox([label, o, b])\n",
    "\n",
    "# Start up the thread\n",
    "t = threading.Thread(target=threadfunc, args=(o,))\n",
    "t.start()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List of Current Streams Jobs\n",
    "\n",
    "Below, the current set of running Streams jobs will be listed, along with their 'health' state.  A job may be unhealthy if it is starting up, or shutting down, or has a run-time error that could not be recovered.  To see more information about the Streams jobs, go to the 'My Instances' panel in the Cloud Pak for Data navigation menu, and select the 'Jobs' tab.  From there, the operator graph of each job can be viewed, and job logs can be downloaded.  Individual jobs can be cancelled there, or all currently running jobs can be cancelled by clicking the button below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the Widgets\n",
    "display(vbox)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
